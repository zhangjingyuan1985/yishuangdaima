#!/usr/bin/env bash

###########################指标库 hive #################################--

nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.tag.Hdfs2HiveJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 6  --executor-memory 5g --driver-memory 1g --conf spark.defalut.parallelism=90 --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h --conf spark.sql.adaptive.enabled=true    --conf spark.yarn.max.executor.failures=24 --conf spark.blacklist.enabled=false    --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  ./jars_zyt/spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/origin/vehicle/gps/taxi/440300/jiaowei/2019/03  t_vehicle_gps_taxi UTF-8"  >>/root/spark-logs/t_vehicle_gps_taxi.log 2>&1  &

nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.tag.Hdfs2HiveJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 5  --executor-memory 8g --driver-memory 1g --conf spark.defalut.parallelism=90 --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h --conf spark.sql.adaptive.enabled=true    --conf spark.yarn.max.executor.failures=24 --conf spark.blacklist.enabled=false    --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  ./jars_zyt/spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/gps/bus/shenzhen/2017/10   t_vehicle_gps_bus UTF-8"  >>/root/spark-logs/hive_t_vehicle_gps_bus.log 2>&1  &

nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.tag.Hdfs2HiveJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 6  --executor-memory 6g --driver-memory 2g  --conf spark.sql.adaptive.enabled=true --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h  --conf spark.blacklist.enabled=false   --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  ./jars_zyt/spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/ic/metro/shenzhen/2017/10 t_transit_people_ic  GBK"  >>/root/spark-logs/t_transit_people_ic.log 2>&1  &

nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.tag.Hdfs2HiveJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 3  --executor-memory 5g --driver-memory 1g --conf spark.defalut.parallelism=75 --conf spark.sql.adaptive.enabled=true --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h  --conf spark.blacklist.enabled=false   --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  ./jars_zyt/spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/msignal/guangdong/home_count/cu/2017/12 t_phone_home_distribute  UTF-8"  >>/root/spark-logs/hive_phone_home_dist.log 2>&1  &


nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.tag.Hdfs2HiveJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 3  --executor-memory 5g --driver-memory 1g --conf spark.defalut.parallelism=75 --conf spark.sql.adaptive.enabled=true --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h  --conf spark.blacklist.enabled=false   --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  ./jars_zyt/spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/msignal/guangdong/work_count/cu/2017/12 t_phone_work_distribute  UTF-8"  >>/root/spark-logs/hive_t_phone_work_distribute.log 2>&1  &


nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.tag.Hdfs2HiveJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 3  --executor-memory 5g --driver-memory 1g --conf spark.defalut.parallelism=75 --conf spark.sql.adaptive.enabled=true --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h  --conf spark.blacklist.enabled=false   --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  ./jars_zyt/spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/msignal/guangdong/home_work_count/cu/2017/12 t_phone_home_work_rela  UTF-8"  >>/root/spark-logs/hive_t_phone_home_work_rela.log 2>&1  &

nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.tag.Hdfs2HiveJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 1  --executor-memory 1g --driver-memory 1g --conf spark.sql.adaptive.enabled=true --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h  --conf spark.blacklist.enabled=false   --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  ./jars_zyt/spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/msignal/guangdong/home_count/cm/2019/03 t_phone_home_distribute_cm  UTF-8"  >>/root/spark-logs/hive_t_phone_home_distribute_cm.log 2>&1  &


nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.tag.Hdfs2HiveJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 1  --executor-memory 1g --driver-memory 1g --conf spark.sql.adaptive.enabled=true --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h  --conf spark.blacklist.enabled=false   --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  ./jars_zyt/spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/origin/phone/msignal/guangdong/work_count/cm/2019/03 t_phone_work_distribute_cm  UTF-8"  >>/root/spark-logs/hive_t_phone_work_distribute_cm.log 2>&1  &


nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.tag.Hdfs2HiveJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 1  --executor-memory 1g --driver-memory 1g --conf spark.sql.adaptive.enabled=true --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h  --conf spark.blacklist.enabled=false   --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  ./jars_zyt/spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/msignal/guangdong/home_work_count/cm/2019/03 t_phone_home_work_rela_cm  UTF-8"  >>/root/spark-logs/hive_t_phone_home_work_rela_cm.log 2>&1  &

nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.tag.Hdfs2HiveJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 3  --executor-memory 5g --driver-memory 1g --conf spark.defalut.parallelism=75 --conf spark.sql.adaptive.enabled=true --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h  --conf spark.blacklist.enabled=false   --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  ./jars_zyt/spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/msignal/guangdong/trip/cu/2017/12 t_phone_trip_distribute  UTF-8"  >>/root/spark-logs/hive_t_phone_trip_distribute.log 2>&1  &


nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.tag.Hdfs2HiveJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 4  --executor-memory 6g --driver-memory 2g  --conf spark.sql.adaptive.enabled=true --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h  --conf spark.blacklist.enabled=false   --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  ./jars_zyt/spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/msignal/guangdong/trip/cm/2019/03 t_phone_trip_distribute_cm  UTF-8"  >>/root/spark-logs/hive_t_phone_trip_distribute_cm.log 2>&1  &


nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.tag.Hdfs2HiveJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 4  --executor-memory 6g --driver-memory 2g  --conf spark.sql.adaptive.enabled=true --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h  --conf spark.blacklist.enabled=false   --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  ./jars_zyt/spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/gps/truck/shenzhen/2017/10 t_vehicle_gps_truck  UTF-8"  >>/root/spark-logs/hive_t_vehicle_gps_truck.log 2>&1  &


nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.std.Hdfs2HiveJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 6  --executor-memory 5g --driver-memory 1g --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h --conf spark.sql.adaptive.enabled=true    --conf spark.yarn.max.executor.failures=24 --conf spark.blacklist.enabled=false    --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  ./jars_zyt/spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/gps/order/shenzhen/2019/06 t_vehicle_gps_order UTF-8"  >>/root/spark-logs/t_vehicle_gps_order.log 2>&1  &