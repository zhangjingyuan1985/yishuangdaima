#!/usr/bin/env bash

##########################################指标库 hbase #######################################
nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.tag.Hdfs2PhoenixJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 6  --executor-memory 5g --driver-memory 1g --conf spark.defalut.parallelism=90 --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h --conf spark.sql.adaptive.enabled=true    --conf spark.yarn.max.executor.failures=24 --conf spark.blacklist.enabled=false    --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  spark-sutpc-1.0-SNAPSHOT-xxx.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/gps/taxi/shenzhen/2018/10  t_vehicle_gps_taxi UTF-8"  >>/root/spark-logs/Hdfs2PhoenixJob_t_vehicle_gps_taxi.log 2>&1  &

nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.tag.Hdfs2PhoenixJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 6  --executor-memory 5g --driver-memory 1g --conf spark.defalut.parallelism=90 --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h --conf spark.sql.adaptive.enabled=true    --conf spark.yarn.max.executor.failures=24 --conf spark.blacklist.enabled=false    --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/gps/bus/2017/10/20  t_vehicle_gps_bus UTF-8"  >>/root/spark-logs/Hdfs2PhoenixJob_t_vehicle_gps_bus.log 2>&1  &

nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.tag.Hdfs2PhoenixJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 6  --executor-memory 6g --driver-memory 2g  --conf spark.sql.adaptive.enabled=true --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h  --conf spark.blacklist.enabled=false   --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/ic/metro/shenzhen/2017/10/* t_transit_people_ic  GBK"  >>/root/spark-logs/Hdfs2PhoenixJob_t_transit_ic.log 2>&1  &


#nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.tag.Hdfs2PhoenixJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 3  --executor-memory 5g --driver-memory 1g --conf spark.defalut.parallelism=75 --conf spark.sql.adaptive.enabled=true --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h  --conf spark.blacklist.enabled=false   --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/weather/2017/10 t_weather_grid_his  UTF-8"  >>/root/spark-logs/Hdfs2PhoenixJob_t_weather_grid_his.log 2>&1  &

nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.tag.Hdfs2PhoenixJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 3  --executor-memory 5g --driver-memory 1g --conf spark.defalut.parallelism=75 --conf spark.sql.adaptive.enabled=true --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h  --conf spark.blacklist.enabled=false   --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/msignal/guangdong/home_count/cu/2017/12 t_phone_home_distribute  UTF-8"  >>/root/spark-logs/Hdfs2PhoenixJob_phone_home_dist.log 2>&1  &


nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.tag.Hdfs2PhoenixJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 3  --executor-memory 5g --driver-memory 1g --conf spark.defalut.parallelism=75 --conf spark.sql.adaptive.enabled=true --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h  --conf spark.blacklist.enabled=false   --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/msignal/guangdong/work_count/cu/2017/12 t_phone_work_distribute  UTF-8"  >>/root/spark-logs/Hdfs2PhoenixJob_phone_work_dist.log 2>&1  &


nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.tag.Hdfs2PhoenixJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 3  --executor-memory 5g --driver-memory 1g --conf spark.defalut.parallelism=75 --conf spark.sql.adaptive.enabled=true --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h  --conf spark.blacklist.enabled=false   --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/msignal/guangdong/home_work_count/cu/2017/12 t_phone_home_work_rela  UTF-8"  >>/root/spark-logs/Hdfs2PhoenixJob_phone_home_work_rela.log 2>&1  &

nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.tag.Hdfs2PhoenixJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 1  --executor-memory 1g --driver-memory 1g --conf spark.sql.adaptive.enabled=true --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h  --conf spark.blacklist.enabled=false   --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/msignal/guangdong/home_count/cm/2019/03 t_phone_home_distribute_cm  UTF-8"  >>/root/spark-logs/Hdfs2PhoenixJob_phone_home_distribute_cm.log 2>&1  &


nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.tag.Hdfs2PhoenixJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 1  --executor-memory 1g --driver-memory 1g --conf spark.sql.adaptive.enabled=true --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h  --conf spark.blacklist.enabled=false   --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/msignal/guangdong/work_count/cm/2019/03 t_phone_work_distribute_cm  UTF-8"  >>/root/spark-logs/Hdfs2PhoenixJob_phone_work_distribute_cm.log 2>&1  &


nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.tag.Hdfs2PhoenixJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 1  --executor-memory 1g --driver-memory 1g --conf spark.sql.adaptive.enabled=true --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h  --conf spark.blacklist.enabled=false   --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/msignal/guangdong/home_work_count/cm/2019/03 t_phone_home_work_rela_cm  UTF-8"  >>/root/spark-logs/Hdfs2PhoenixJob_phone_home_work_rela_cm.log 2>&1  &

nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.tag.Hdfs2PhoenixJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 3  --executor-memory 5g --driver-memory 1g --conf spark.defalut.parallelism=75 --conf spark.sql.adaptive.enabled=true --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h  --conf spark.blacklist.enabled=false   --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/msignal/guangdong/trip/cu/2017/12 t_phone_trip_distribute  UTF-8"  >>/root/spark-logs/Hdfs2PhoenixJob_t_phone_trip_distribute.log 2>&1  &


nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.tag.Hdfs2PhoenixJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 4  --executor-memory 6g --driver-memory 2g  --conf spark.sql.adaptive.enabled=true --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h  --conf spark.blacklist.enabled=false   --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/msignal/guangdong/trip/cm/2019/03 t_phone_trip_distribute_cm  UTF-8"  >>/root/spark-logs/Hdfs2PhoenixJob_t_phone_trip_distribute_cm.log 2>&1  &


nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.tag.Hdfs2PhoenixJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 4  --executor-memory 6g --driver-memory 2g  --conf spark.sql.adaptive.enabled=true --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h  --conf spark.blacklist.enabled=false   --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/gps/truck/2017/10 t_vehicle_gps_truck  UTF-8"  >>/root/spark-logs/Hdfs2PhoenixJob_t_vehicle_gps_truck.log 2>&1  &

#####################标准库 hive##################---

nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.std.Hdfs2HiveJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 5  --executor-memory 6g --driver-memory 1g --conf spark.defalut.parallelism=90 --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h --conf spark.sql.adaptive.enabled=true    --conf spark.yarn.max.executor.failures=24 --conf spark.blacklist.enabled=false    --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/origin/vehicle/gps/bus/440300/2017/10 s_vehicle_gps_bus UTF-8"  >>/root/spark-logs/s_vehicle_gps_bus.log 2>&1  &



nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.std.Hdfs2HiveJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 5  --executor-memory 6g --driver-memory 1g --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h --conf spark.sql.adaptive.enabled=true    --conf spark.yarn.max.executor.failures=24 --conf spark.blacklist.enabled=false    --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/gps/taxi/shenzhen/2019/07 s_vehicle_gps_taxi UTF-8"  >>/root/spark-logs/s_vehicle_gps_taxi.log 2>&1  &


nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.std.Hdfs2HiveJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 5  --executor-memory 7g --driver-memory 1g --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h --conf spark.sql.adaptive.enabled=true    --conf spark.yarn.max.executor.failures=24 --conf spark.blacklist.enabled=false    --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/gps/baidu/shenzhen/2019/07/01 s_vehicle_gps_drive UTF-8"  >>/root/spark-logs/s_vehicle_gps_drive.log 2>&1  &

#复制到指标库
nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.std.Hdfs2HiveJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 5  --executor-memory 8g --driver-memory 1g  --conf spark.yarn.executor.memoryOverhead=1g  --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h --conf spark.sql.adaptive.enabled=true    --conf spark.yarn.max.executor.failures=24 --conf spark.blacklist.enabled=false    --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/gps/baidu/shenzhen/2019/07/01 s_vehicle_gps_drive UTF-8"  >>/root/spark-logs/s_vehicle_gps_drive.log 2>&1  &



nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.std.Hdfs2HiveJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 6  --executor-memory 5g --driver-memory 1g --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h --conf spark.sql.adaptive.enabled=true    --conf spark.yarn.max.executor.failures=24 --conf spark.blacklist.enabled=false    --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/gps/order/shenzhen/2019/06 s_vehicle_gps_order UTF-8"  >>/root/spark-logs/s_vehicle_gps_order.log 2>&1  &


nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.std.Hdfs2HiveJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 5  --executor-memory 6g --driver-memory 3g --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h --conf spark.sql.adaptive.enabled=true    --conf spark.yarn.max.executor.failures=24 --conf spark.blacklist.enabled=false    --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/gps/truck/shenzhen/2019/06 s_vehicle_gps_others_truck UTF-8"  >>/root/spark-logs/s_vehicle_gps_others_truck.log 2>&1  &


nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.std.Hdfs2HiveJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 5  --executor-memory 6g --driver-memory 3g --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h --conf spark.sql.adaptive.enabled=true    --conf spark.yarn.max.executor.failures=24 --conf spark.blacklist.enabled=false    --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/ic/metro/shenzhen/2017/10 s_transit_people_ic GBK"  >>/root/spark-logs/s_transit_people_ic.log 2>&1  &

nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.std.Hdfs2HiveJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 5  --executor-memory 6g --driver-memory 1g --conf spark.defalut.parallelism=90 --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h --conf spark.sql.adaptive.enabled=true    --conf spark.yarn.max.executor.failures=24 --conf spark.blacklist.enabled=false    --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/share_bike/shenzhen/2018/10/a_share_data.sql s_bike_switch_lock UTF-8"  >>/root/spark-logs/s_bike_switch_lock.log 2>&1  &

#复制到指标库
nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.std.Hdfs2HiveJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 5  --executor-memory 6g --driver-memory 1g --conf spark.defalut.parallelism=90 --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h --conf spark.sql.adaptive.enabled=true    --conf spark.yarn.max.executor.failures=24 --conf spark.blacklist.enabled=false    --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/share_bike/switch_lock/shenzhen/2018/10  t_bike_switch_lock UTF-8"  >>/root/spark-logs/t_bike_switch_lock.log 2>&1  &


nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.std.Hdfs2HiveJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 4  --executor-memory 3g --driver-memory 1g --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h --conf spark.sql.adaptive.enabled=true    --conf spark.yarn.max.executor.failures=24 --conf spark.blacklist.enabled=false    --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/gps/truck/shenzhen/2019/06  s_vehicle_gps_truck UTF-8"  >>/root/spark-logs/s_vehicle_gps_truck.log 2>&1  &


nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.std.Hdfs2HiveJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 4  --executor-memory 3g --driver-memory 1g --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h --conf spark.sql.adaptive.enabled=true    --conf spark.yarn.max.executor.failures=24 --conf spark.blacklist.enabled=false    --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/gps/charter/shenzhen/2019/06,/data/gps/coach/shenzhen/2019/06,/data/gps/danger/shenzhen/2019/06,/data/gps/driving/shenzhen/2019/06,/data/gps/dumper/shenzhen/2019/06,/data/gps/others/shenzhen/2019/06  s_vehicle_gps_others UTF-8"  >>/root/spark-logs/s_vehicle_gps_others.log 2>&1  &


nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.std.Hdfs2HiveJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 4  --executor-memory 3g --driver-memory 1g --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h --conf spark.sql.adaptive.enabled=true    --conf spark.yarn.max.executor.failures=24 --conf spark.blacklist.enabled=false    --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/share_bike/loc/mobike/shenzhen/2017/04  s_bike_loc UTF-8"  >>/root/spark-logs/s_bike_loc.log 2>&1  &


nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.std.Hdfs2HiveJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 5  --executor-memory 6g --driver-memory 1g --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h --conf spark.sql.adaptive.enabled=true    --conf spark.yarn.max.executor.failures=24 --conf spark.blacklist.enabled=false    --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/msignal/guangdong/activity/cu/2017/12  s_phone_inter_activity UTF-8"  >>/root/spark-logs/s_phone_inter_activity.log 2>&1  &


nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.std.Hdfs2HiveJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 5  --executor-memory 6g --driver-memory 1g --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h --conf spark.sql.adaptive.enabled=true    --conf spark.yarn.max.executor.failures=24 --conf spark.blacklist.enabled=false    --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/msignal/guangdong/activity/cm/2019/03   s_phone_inter_activity_cm UTF-8"  >>/root/spark-logs/s_phone_inter_activity_cm.log 2>&1  &



nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.std.Hdfs2HiveJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 5  --executor-memory 6g --driver-memory 1g --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h --conf spark.sql.adaptive.enabled=true    --conf spark.yarn.max.executor.failures=24 --conf spark.blacklist.enabled=false    --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/origin/road/recognition/440300/2018/10-11   s_road_plate_recognition2018 UTF-8"  >>/root/spark-logs/s_road_plate_recognition2018.log 2>&1  &

nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.std.Hdfs2HiveJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 5  --executor-memory 6g --driver-memory 1g --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h --conf spark.sql.adaptive.enabled=true    --conf spark.yarn.max.executor.failures=24 --conf spark.blacklist.enabled=false    --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/origin/road/recognition/440300/2019/09   s_road_plate_recognition2019 UTF-8"  >>/root/spark-logs/s_road_plate_recognition2019.log 2>&1  &



###############################数据量统计
10 12 * * * nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.std.HdfsStatJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 5  --executor-memory 6g --driver-memory 3g --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h --conf spark.sql.adaptive.enabled=true    --conf spark.yarn.max.executor.failures=24 --conf spark.blacklist.enabled=false    --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  spark-sutpc-1.0-SNAPSHOT.jar `date -d "1 days ago" +\%Y\%m\%d`"  >>/root/spark-logs/HdfsStatJob.log 2>&1  &




###########################指标库 hive #################################--

nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.tag.Hdfs2HiveJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 6  --executor-memory 5g --driver-memory 1g --conf spark.defalut.parallelism=90 --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h --conf spark.sql.adaptive.enabled=true    --conf spark.yarn.max.executor.failures=24 --conf spark.blacklist.enabled=false    --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/gps/taxi/shenzhen/2018/10  t_vehicle_gps_taxi UTF-8"  >>/root/spark-logs/t_vehicle_gps_taxi.log 2>&1  &

nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.tag.Hdfs2HiveJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 5  --executor-memory 8g --driver-memory 1g --conf spark.defalut.parallelism=90 --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h --conf spark.sql.adaptive.enabled=true    --conf spark.yarn.max.executor.failures=24 --conf spark.blacklist.enabled=false    --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/gps/bus/shenzhen/2017/10   t_vehicle_gps_bus UTF-8"  >>/root/spark-logs/hive_t_vehicle_gps_bus.log 2>&1  &

nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.tag.Hdfs2HiveJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 6  --executor-memory 6g --driver-memory 2g  --conf spark.sql.adaptive.enabled=true --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h  --conf spark.blacklist.enabled=false   --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/ic/metro/shenzhen/2017/10 t_transit_people_ic  GBK"  >>/root/spark-logs/t_transit_people_ic.log 2>&1  &

####

nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.tag.Hdfs2HiveJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 3  --executor-memory 5g --driver-memory 1g --conf spark.defalut.parallelism=75 --conf spark.sql.adaptive.enabled=true --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h  --conf spark.blacklist.enabled=false   --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/msignal/guangdong/home_count/cu/2017/12 t_phone_home_distribute  UTF-8"  >>/root/spark-logs/hive_phone_home_dist.log 2>&1  &


nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.tag.Hdfs2HiveJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 3  --executor-memory 5g --driver-memory 1g --conf spark.defalut.parallelism=75 --conf spark.sql.adaptive.enabled=true --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h  --conf spark.blacklist.enabled=false   --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/msignal/guangdong/work_count/cu/2017/12 t_phone_work_distribute  UTF-8"  >>/root/spark-logs/hive_t_phone_work_distribute.log 2>&1  &


nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.tag.Hdfs2HiveJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 3  --executor-memory 5g --driver-memory 1g --conf spark.defalut.parallelism=75 --conf spark.sql.adaptive.enabled=true --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h  --conf spark.blacklist.enabled=false   --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/msignal/guangdong/home_work_count/cu/2017/12 t_phone_home_work_rela  UTF-8"  >>/root/spark-logs/hive_t_phone_home_work_rela.log 2>&1  &

nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.tag.Hdfs2HiveJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 1  --executor-memory 1g --driver-memory 1g --conf spark.sql.adaptive.enabled=true --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h  --conf spark.blacklist.enabled=false   --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/msignal/guangdong/home_count/cm/2019/03 t_phone_home_distribute_cm  UTF-8"  >>/root/spark-logs/hive_t_phone_home_distribute_cm.log 2>&1  &


nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.tag.Hdfs2HiveJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 1  --executor-memory 1g --driver-memory 1g --conf spark.sql.adaptive.enabled=true --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h  --conf spark.blacklist.enabled=false   --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/msignal/guangdong/work_count/cm/2019/03 t_phone_work_distribute_cm  UTF-8"  >>/root/spark-logs/hive_t_phone_work_distribute_cm.log 2>&1  &


nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.tag.Hdfs2HiveJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 1  --executor-memory 1g --driver-memory 1g --conf spark.sql.adaptive.enabled=true --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h  --conf spark.blacklist.enabled=false   --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/msignal/guangdong/home_work_count/cm/2019/03 t_phone_home_work_rela_cm  UTF-8"  >>/root/spark-logs/hive_t_phone_home_work_rela_cm.log 2>&1  &

nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.tag.Hdfs2HiveJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 3  --executor-memory 5g --driver-memory 1g --conf spark.defalut.parallelism=75 --conf spark.sql.adaptive.enabled=true --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h  --conf spark.blacklist.enabled=false   --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/msignal/guangdong/trip/cu/2017/12 t_phone_trip_distribute  UTF-8"  >>/root/spark-logs/hive_t_phone_trip_distribute.log 2>&1  &


nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.tag.Hdfs2HiveJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 4  --executor-memory 6g --driver-memory 2g  --conf spark.sql.adaptive.enabled=true --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h  --conf spark.blacklist.enabled=false   --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/msignal/guangdong/trip/cm/2019/03 t_phone_trip_distribute_cm  UTF-8"  >>/root/spark-logs/hive_t_phone_trip_distribute_cm.log 2>&1  &


nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.tag.Hdfs2HiveJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 4  --executor-memory 6g --driver-memory 2g  --conf spark.sql.adaptive.enabled=true --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h  --conf spark.blacklist.enabled=false   --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/gps/truck/shenzhen/2017/10 t_vehicle_gps_truck  UTF-8"  >>/root/spark-logs/hive_t_vehicle_gps_truck.log 2>&1  &


nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.std.Hdfs2HiveJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 6  --executor-memory 5g --driver-memory 1g --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h --conf spark.sql.adaptive.enabled=true    --conf spark.yarn.max.executor.failures=24 --conf spark.blacklist.enabled=false    --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/gps/order/shenzhen/2019/06 t_vehicle_gps_order UTF-8"  >>/root/spark-logs/t_vehicle_gps_order.log 2>&1  &


##############################################################


--conf spark.yarn.executor.memoryOverhead=4096

nohup su - hdfs -c "spark-submit --master yarn  --deploy-mode client --class com.sutpc.bigdata.job.batch.Hdfs2PhoenixJob --files /etc/hbase/conf/hbase-site.xml --executor-cores 5 --num-executors 3  --executor-memory 5g --driver-memory 3g --conf spark.defalut.parallelism=75 --conf spark.sql.adaptive.enabled=true --conf spark.yarn.maxAppAttempts=4  --conf spark.task.maxFailures=8     --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.executor.failuresValidityInterval=1h --conf spark.sql.adaptive.enabled=true    --conf spark.yarn.max.executor.failures=24 --conf  spark.streaming.backpressure.enabled=true  --conf spark.blacklist.enabled=false   --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"  spark-sutpc-1.0-SNAPSHOT.jar '10.10.201.8,10.10.201.11,10.10.201.12:2181' /data/ic/metro/shenzhen/2017/10/* t_transit_people_ic"  >>/root/spark-logs/Hdfs2PhoenixJob_t_transit_ic.log 2>&1  &